
# `zrygan/nlp/machine_translation`

Machine translating one source language to a target language using the Seq-to-Seq toolkit [fairseq](https://github.com/facebookresearch/fairseq).

This program requires `uv` to run a fast python package and project manager, 

To run the model, use perform `pip install uv`. Then, change virtual env: `source .venv/bin/activate`.

Any actions about module installations and running python files, run `uv` such as:
`uv run --active python -c "import fai"`.

To download the latest packages, perform `uv sync`.

- [`zrygan/nlp/machine_translation`](#zrygannlpmachine_translation)
  - [Project Files](#project-files)
  - [Usage](#usage)
    - [Preprocess](#preprocess)
    - [Training](#training)
    - [Evaluation](#evaluation)
    - [Interactive](#interactive)
  - [Configuration](#configuration)
  - [Model Specifications](#model-specifications)
  - [Declaration of AI Use](#declaration-of-ai-use)

## Project Files

```txt
├─checkpoints
├─data
├─data-bin
├─results
├─scripts
├─all_languages.txt
├─pyproject.toml
└─special_token.txt

```

## Usage

Make sure to activate the virtual directory first.

There's a preconfigured main file to run if lazy: `./fairseq_main.bash`.

### Preprocess

First, preprocess all the verses data from `bible_cleaning/parallel_corpus/by_verses` to unigram.

Go to `/scripts` or perform `cd ./scripts`. Assuming that you are executing in that folder, execute:

```bash
uv run --active python training_data_creation.py
```

Then, convert raw data to unigram format.

Change directory to `/unigram_preprocess` or do `cd ./unigram_preprocess` and execute the following lines of code:

```bash
./unigram_conversion
```

Make sure that it has permissions, if not, perform `chmod 777 <filename>` in the same directory. Executing this setups the folders and files to run three python files for unigram generation, encoding, and fairseq preprocessing.

> NOTE: you can configure the SRC and DST language. Default as of moment is tagalog and cebuano.
> Change the settings for which unigram via unigram.bash file
> SRC_LANG="ceb"
> TGT_LANG="tgl

### Training

Afterwards, you can start training. Make sure you have the right uv modules for training.

Change directory to `/train`, and choose a machine translation method to train.

> NOTE: if you want to configure a new format, save the previous version training bash script and the evaulation results in `fairseq_mt/evaluation/<user_defined_mt_name>`

SRC --> DST: `./train-src-dest.bash`

SRC --> PIVOT --> DST: `./train-src-pivot-dest.bash`

While 10 minutes while training.

Afterwards, it should produce a checkpoint at `/checkpoint`.

### Evaluation

Change directory to the parent directory and perform evaluation.

```bash
./fairseq_evaluate.bash
```

Check `./results` for the BLEU score training, test, and valid data

### Interactive

> [!wip] Work in Progress

You can translate directly from the machine translation model. Check out `/interaction` to check how to translate from source to destination language.

You can also translate with a middle pivot language.

## Configuration

## Model Specifications

## Declaration of AI Use

The author used ChatGPT-5 to assist with language editing and improving
the clarity of the manuscript. All content generated by the AI was
carefully reviewed and edited by the authors. The authors take full
responsibility for the accuracy and integrity of the final manuscript.
