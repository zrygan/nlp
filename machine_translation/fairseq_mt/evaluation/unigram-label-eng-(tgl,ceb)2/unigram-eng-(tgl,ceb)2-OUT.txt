2025-11-22 07:10:56 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/augmentation/aug2', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 2, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=42, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe='sentencepiece', optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_iwslt_de_en', max_epoch=50, max_update=0, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='../../checkpoints/augmentation/aug2', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=2, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='../../data-bin/augmentation/eng-tgl/bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='sentencepiece', eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, sentencepiece_model='unigram', sentencepiece_enable_sampling=False, sentencepiece_alpha=None, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=8000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=512, encoder_ffn_embed_dim=1024, encoder_attention_heads=4, encoder_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=1024, decoder_attention_heads=4, decoder_layers=6, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_iwslt_de_en'), 'task': {'_name': 'translation', 'data': '../../data-bin/augmentation/eng-tgl/bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 8000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': 'unigram', 'sentencepiece_enable_sampling': False, 'sentencepiece_alpha': None}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2025-11-22 07:10:57 | INFO | fairseq.tasks.translation | [eng] dictionary: 15456 types
2025-11-22 07:10:57 | INFO | fairseq.tasks.translation | [tgl] dictionary: 15456 types
2025-11-22 07:10:57 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(15456, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(15456, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=15456, bias=False)
  )
)
2025-11-22 07:10:57 | INFO | fairseq_cli.train | task: TranslationTask
2025-11-22 07:10:57 | INFO | fairseq_cli.train | model: TransformerModel
2025-11-22 07:10:57 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2025-11-22 07:10:57 | INFO | fairseq_cli.train | num. shared model params: 55,285,760 (num. trained: 55,285,760)
2025-11-22 07:10:57 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2025-11-22 07:10:57 | INFO | fairseq.data.data_utils | loaded 3,699 examples from: ../../data-bin/augmentation/eng-tgl/bin/valid.eng-tgl.eng
2025-11-22 07:10:57 | INFO | fairseq.data.data_utils | loaded 3,699 examples from: ../../data-bin/augmentation/eng-tgl/bin/valid.eng-tgl.tgl
2025-11-22 07:10:57 | INFO | fairseq.tasks.translation | ../../data-bin/augmentation/eng-tgl/bin valid eng-tgl 3699 examples
2025-11-22 07:10:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-11-22 07:10:57 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 3.680 GB ; name = NVIDIA GeForce RTX 3050 Ti Laptop GPU   
2025-11-22 07:10:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-11-22 07:10:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2025-11-22 07:10:57 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2025-11-22 07:10:57 | INFO | fairseq.trainer | Preparing to load checkpoint ../../checkpoints/augmentation/aug2/checkpoint_last.pt
2025-11-22 07:10:57 | INFO | fairseq.trainer | No existing checkpoint found ../../checkpoints/augmentation/aug2/checkpoint_last.pt
2025-11-22 07:10:57 | INFO | fairseq.trainer | loading train data for epoch 1
2025-11-22 07:10:57 | INFO | fairseq.data.data_utils | loaded 66,606 examples from: ../../data-bin/augmentation/eng-tgl/bin/train.eng-tgl.eng
2025-11-22 07:10:57 | INFO | fairseq.data.data_utils | loaded 66,606 examples from: ../../data-bin/augmentation/eng-tgl/bin/train.eng-tgl.tgl
2025-11-22 07:10:57 | INFO | fairseq.tasks.translation | ../../data-bin/augmentation/eng-tgl/bin train eng-tgl 66606 examples
2025-11-22 07:10:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:10:57 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2025-11-22 07:10:57 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2025-11-22 07:10:57 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2025-11-22 07:10:58 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2025-11-22 07:10:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:10:58 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2025-11-22 07:10:58 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2025-11-22 07:10:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2025-11-22 07:10:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 001:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:10:58 | INFO | fairseq.trainer | begin training epoch 1
2025-11-22 07:10:58 | INFO | fairseq_cli.train | Start iterating over samples
/home/logyre22/Projects/nlp/machine_translation/fairseq_mt/.venv/lib/python3.10/site-packages/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/logyre22/Projects/nlp/machine_translation/fairseq_mt/.venv/lib/python3.10/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2025-11-22 07:10:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
epoch 001: 100%|▉| 484/485 [01:14<00:00,  6.45it/s, loss=8.997, nll_loss=8.279, ppl=310.67, wps=21381.5, ups=6.5, wpb=32892025-11-22 07:12:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:12:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:12:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.24 | nll_loss 7.321 | ppl 159.91 | bleu 4.2 | wps 4392 | wpb 1765.3 | bsz 73.9 | num_updates 484
2025-11-22 07:12:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 484 updates
2025-11-22 07:12:33 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:12:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 1 @ 484 updates, score 4.2) (writing took 0.9770096259890124 seconds)
2025-11-22 07:12:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)                               
2025-11-22 07:12:34 | INFO | train | epoch 001 | loss 10.383 | nll_loss 9.854 | ppl 925.4 | wps 16470.9 | ups 5.09 | wpb 3239.1 | bsz 137.4 | num_updates 484 | lr 3.0344e-05 | gnorm 1.969 | clip 83.1 | loss_scale 64 | train_wall 73 | gb_free 1.5 | wall 97
2025-11-22 07:12:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:12:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 002:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:12:34 | INFO | fairseq.trainer | begin training epoch 2
2025-11-22 07:12:34 | INFO | fairseq_cli.train | Start iterating over samples
epoch 002: 100%|▉| 484/485 [01:14<00:00,  6.39it/s, loss=7.542, nll_loss=6.539, ppl=92.96, wps=20859.8, ups=6.56, wpb=31792025-11-22 07:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:13:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:14:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.244 | nll_loss 6.17 | ppl 71.99 | bleu 5.11 | wps 3422.7 | wpb 1765.3 | bsz 73.9 | num_updates 969 | best_bleu 5.11
2025-11-22 07:14:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 969 updates
2025-11-22 07:14:14 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:14:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:14:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 2 @ 969 updates, score 5.11) (writing took 1.0733807240030728 seconds)
2025-11-22 07:14:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)                               
2025-11-22 07:14:15 | INFO | train | epoch 002 | loss 7.738 | nll_loss 6.77 | ppl 109.13 | wps 15505.3 | ups 4.78 | wpb 3240.4 | bsz 137.3 | num_updates 969 | lr 6.06504e-05 | gnorm 1.071 | clip 42.1 | loss_scale 64 | train_wall 73 | gb_free 1.3 | wall 198
2025-11-22 07:14:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:14:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 003:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:14:15 | INFO | fairseq.trainer | begin training epoch 3
2025-11-22 07:14:15 | INFO | fairseq_cli.train | Start iterating over samples
epoch 003: 100%|▉| 484/485 [01:14<00:00,  6.56it/s, loss=6.91, nll_loss=5.809, ppl=56.08, wps=20923.7, ups=6.42, wpb=3258.2025-11-22 07:15:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:15:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:15:57 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.8 | nll_loss 5.665 | ppl 50.73 | bleu 5.08 | wps 3275.3 | wpb 1765.3 | bsz 73.9 | num_updates 1454 | best_bleu 5.11
2025-11-22 07:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1454 updates
2025-11-22 07:15:57 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)                               
2025-11-22 07:15:57 | INFO | train | epoch 003 | loss 7.064 | nll_loss 5.987 | ppl 63.43 | wps 15465.5 | ups 4.77 | wpb 3240.4 | bsz 137.3 | num_updates 1454 | lr 9.09568e-05 | gnorm 0.944 | clip 26.8 | loss_scale 64 | train_wall 74 | gb_free 1.4 | wall 299
2025-11-22 07:15:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:15:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 004:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:15:57 | INFO | fairseq.trainer | begin training epoch 4
2025-11-22 07:15:57 | INFO | fairseq_cli.train | Start iterating over samples
epoch 004: 100%|▉| 484/485 [01:14<00:00,  6.61it/s, loss=6.584, nll_loss=5.435, ppl=43.26, wps=21759.7, ups=6.46, wpb=33672025-11-22 07:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:17:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:17:37 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.44 | nll_loss 5.257 | ppl 38.24 | bleu 7.17 | wps 3455.2 | wpb 1765.3 | bsz 73.9 | num_updates 1939 | best_bleu 7.17
2025-11-22 07:17:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1939 updates
2025-11-22 07:17:37 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:17:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 4 @ 1939 updates, score 7.17) (writing took 1.0600754240294918 seconds)
2025-11-22 07:17:38 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)                               
2025-11-22 07:17:38 | INFO | train | epoch 004 | loss 6.675 | nll_loss 5.539 | ppl 46.49 | wps 15523 | ups 4.79 | wpb 3240.4 | bsz 137.3 | num_updates 1939 | lr 0.000121263 | gnorm 0.9 | clip 26 | loss_scale 64 | train_wall 74 | gb_free 1.4 | wall 401
2025-11-22 07:17:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:17:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 005:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:17:38 | INFO | fairseq.trainer | begin training epoch 5
2025-11-22 07:17:38 | INFO | fairseq_cli.train | Start iterating over samples
epoch 005: 100%|▉| 484/485 [01:14<00:00,  6.52it/s, loss=6.281, nll_loss=5.088, ppl=34.01, wps=20870.5, ups=6.61, wpb=31552025-11-22 07:18:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:18:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:19:15 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.199 | nll_loss 4.943 | ppl 30.77 | bleu 7.22 | wps 3966.9 | wpb 1765.3 | bsz 73.9 | num_updates 2424 | best_bleu 7.22
2025-11-22 07:19:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2424 updates
2025-11-22 07:19:15 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:19:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 5 @ 2424 updates, score 7.22) (writing took 0.9513024899642915 seconds)
2025-11-22 07:19:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)                               
2025-11-22 07:19:16 | INFO | train | epoch 005 | loss 6.367 | nll_loss 5.187 | ppl 36.43 | wps 16074.7 | ups 4.96 | wpb 3240.4 | bsz 137.3 | num_updates 2424 | lr 0.00015157 | gnorm 0.895 | clip 22.7 | loss_scale 64 | train_wall 73 | gb_free 1.5 | wall 499
2025-11-22 07:19:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:19:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 006:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:19:16 | INFO | fairseq.trainer | begin training epoch 6
2025-11-22 07:19:16 | INFO | fairseq_cli.train | Start iterating over samples
epoch 006: 100%|▉| 484/485 [01:14<00:00,  6.52it/s, loss=6.024, nll_loss=4.795, ppl=27.76, wps=21546.3, ups=6.52, wpb=33052025-11-22 07:20:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:20:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:20:55 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.903 | nll_loss 4.604 | ppl 24.32 | bleu 9.57 | wps 3612.5 | wpb 1765.3 | bsz 73.9 | num_updates 2909 | best_bleu 9.57
2025-11-22 07:20:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2909 updates
2025-11-22 07:20:55 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:20:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:20:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 6 @ 2909 updates, score 9.57) (writing took 1.648951143026352 seconds)
2025-11-22 07:20:56 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)                               
2025-11-22 07:20:56 | INFO | train | epoch 006 | loss 6.089 | nll_loss 4.87 | ppl 29.23 | wps 15619.9 | ups 4.82 | wpb 3240.4 | bsz 137.3 | num_updates 2909 | lr 0.000181876 | gnorm 0.875 | clip 18.8 | loss_scale 64 | train_wall 73 | gb_free 1.6 | wall 599
2025-11-22 07:20:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:20:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 007:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:20:56 | INFO | fairseq.trainer | begin training epoch 7
2025-11-22 07:20:56 | INFO | fairseq_cli.train | Start iterating over samples
epoch 007: 100%|▉| 484/485 [01:14<00:00,  6.39it/s, loss=5.797, nll_loss=4.536, ppl=23.2, wps=20944.7, ups=6.44, wpb=3249.2025-11-22 07:22:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:22:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:22:35 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.718 | nll_loss 4.388 | ppl 20.94 | bleu 10.44 | wps 3694.8 | wpb 1765.3 | bsz 73.9 | num_updates 3394 | best_bleu 10.44
2025-11-22 07:22:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 3394 updates
2025-11-22 07:22:35 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:22:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:22:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 7 @ 3394 updates, score 10.44) (writing took 2.152712718001567 seconds)
2025-11-22 07:22:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)                               
2025-11-22 07:22:37 | INFO | train | epoch 007 | loss 5.836 | nll_loss 4.581 | ppl 23.93 | wps 15601.8 | ups 4.81 | wpb 3240.4 | bsz 137.3 | num_updates 3394 | lr 0.000212183 | gnorm 0.854 | clip 19.6 | loss_scale 64 | train_wall 74 | gb_free 1.4 | wall 700
2025-11-22 07:22:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:22:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 008:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:22:37 | INFO | fairseq.trainer | begin training epoch 8
2025-11-22 07:22:37 | INFO | fairseq_cli.train | Start iterating over samples
epoch 008: 100%|▉| 484/485 [01:14<00:00,  6.45it/s, loss=5.608, nll_loss=4.318, ppl=19.95, wps=21044.7, ups=6.58, wpb=31982025-11-22 07:23:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:23:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:24:15 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.554 | nll_loss 4.182 | ppl 18.15 | bleu 11.36 | wps 3713.9 | wpb 1765.3 | bsz 73.9 | num_updates 3879 | best_bleu 11.36
2025-11-22 07:24:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3879 updates
2025-11-22 07:24:15 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:24:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:24:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 8 @ 3879 updates, score 11.36) (writing took 1.350283631007187 seconds)
2025-11-22 07:24:17 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)                               
2025-11-22 07:24:17 | INFO | train | epoch 008 | loss 5.628 | nll_loss 4.341 | ppl 20.27 | wps 15738.3 | ups 4.86 | wpb 3240.4 | bsz 137.3 | num_updates 3879 | lr 0.000242489 | gnorm 0.854 | clip 18.6 | loss_scale 64 | train_wall 74 | gb_free 1.5 | wall 800
2025-11-22 07:24:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:24:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 009:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:24:17 | INFO | fairseq.trainer | begin training epoch 9
2025-11-22 07:24:17 | INFO | fairseq_cli.train | Start iterating over samples
epoch 009: 100%|▉| 484/485 [01:14<00:00,  6.94it/s, loss=5.444, nll_loss=4.127, ppl=17.47, wps=20690.1, ups=6.45, wpb=32052025-11-22 07:25:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:25:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:25:55 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.433 | nll_loss 4.045 | ppl 16.51 | bleu 11.85 | wps 3756 | wpb 1765.3 | bsz 73.9 | num_updates 4364 | best_bleu 11.85
2025-11-22 07:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 4364 updates
2025-11-22 07:25:55 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 9 @ 4364 updates, score 11.85) (writing took 1.3607463770313188 seconds)
2025-11-22 07:25:56 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)                               
2025-11-22 07:25:56 | INFO | train | epoch 009 | loss 5.45 | nll_loss 4.135 | ppl 17.57 | wps 15790.3 | ups 4.87 | wpb 3240.4 | bsz 137.3 | num_updates 4364 | lr 0.000272795 | gnorm 0.842 | clip 15.9 | loss_scale 64 | train_wall 73 | gb_free 1.5 | wall 899
2025-11-22 07:25:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:25:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 010:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:25:56 | INFO | fairseq.trainer | begin training epoch 10
2025-11-22 07:25:56 | INFO | fairseq_cli.train | Start iterating over samples
epoch 010: 100%|▉| 484/485 [01:14<00:00,  6.41it/s, loss=5.343, nll_loss=4.009, ppl=16.1, wps=21034.4, ups=6.58, wpb=3198,2025-11-22 07:27:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:27:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:27:33 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.373 | nll_loss 3.969 | ppl 15.66 | bleu 11.83 | wps 3915.8 | wpb 1765.3 | bsz 73.9 | num_updates 4849 | best_bleu 11.85
2025-11-22 07:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4849 updates
2025-11-22 07:27:33 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_11.8301.pt
2025-11-22 07:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_11.8301.pt
2025-11-22 07:27:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint.best_bleu_11.8301.pt (epoch 10 @ 4849 updates, score 11.83) (writing took 0.6722837300039828 seconds)
2025-11-22 07:27:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)                              
2025-11-22 07:27:34 | INFO | train | epoch 010 | loss 5.288 | nll_loss 3.949 | ppl 15.44 | wps 16081.1 | ups 4.96 | wpb 3240.4 | bsz 137.3 | num_updates 4849 | lr 0.000303102 | gnorm 0.837 | clip 16.3 | loss_scale 64 | train_wall 73 | gb_free 1.5 | wall 997
2025-11-22 07:27:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:27:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 011:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:27:34 | INFO | fairseq.trainer | begin training epoch 11
2025-11-22 07:27:34 | INFO | fairseq_cli.train | Start iterating over samples
epoch 011: 100%|▉| 484/485 [01:14<00:00,  6.71it/s, loss=5.171, nll_loss=3.809, ppl=14.02, wps=21459.5, ups=6.56, wpb=32732025-11-22 07:28:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:28:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:29:13 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.288 | nll_loss 3.861 | ppl 14.53 | bleu 13.36 | wps 3549.7 | wpb 1765.3 | bsz 73.9 | num_updates 5334 | best_bleu 13.36
2025-11-22 07:29:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 5334 updates
2025-11-22 07:29:13 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:29:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:29:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 11 @ 5334 updates, score 13.36) (writing took 1.3800617530359887 seconds)
2025-11-22 07:29:15 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)                              
2025-11-22 07:29:15 | INFO | train | epoch 011 | loss 5.143 | nll_loss 3.78 | ppl 13.74 | wps 15615.3 | ups 4.82 | wpb 3240.4 | bsz 137.3 | num_updates 5334 | lr 0.000333408 | gnorm 0.836 | clip 18.1 | loss_scale 64 | train_wall 73 | gb_free 1.3 | wall 1098
2025-11-22 07:29:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:29:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 012:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:29:15 | INFO | fairseq.trainer | begin training epoch 12
2025-11-22 07:29:15 | INFO | fairseq_cli.train | Start iterating over samples
epoch 012: 100%|▉| 484/485 [01:14<00:00,  6.37it/s, loss=5.044, nll_loss=3.662, ppl=12.66, wps=21156.3, ups=6.61, wpb=31992025-11-22 07:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:30:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:30:52 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.221 | nll_loss 3.775 | ppl 13.69 | bleu 13.54 | wps 3845.6 | wpb 1765.3 | bsz 73.9 | num_updates 5819 | best_bleu 13.54
2025-11-22 07:30:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 5819 updates
2025-11-22 07:30:52 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 12 @ 5819 updates, score 13.54) (writing took 1.3817919749999419 seconds)
2025-11-22 07:30:54 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)                              
2025-11-22 07:30:54 | INFO | train | epoch 012 | loss 5.013 | nll_loss 3.629 | ppl 12.37 | wps 15906.4 | ups 4.91 | wpb 3240.4 | bsz 137.3 | num_updates 5819 | lr 0.000363715 | gnorm 0.833 | clip 15.5 | loss_scale 64 | train_wall 73 | gb_free 1.3 | wall 1196
2025-11-22 07:30:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:30:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 013:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:30:54 | INFO | fairseq.trainer | begin training epoch 13
2025-11-22 07:30:54 | INFO | fairseq_cli.train | Start iterating over samples
epoch 013: 100%|▉| 484/485 [01:14<00:00,  6.34it/s, loss=4.898, nll_loss=3.491, ppl=11.25, wps=21312.4, ups=6.54, wpb=32592025-11-22 07:32:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:32:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:32:31 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.18 | nll_loss 3.732 | ppl 13.28 | bleu 13.57 | wps 3854.9 | wpb 1765.3 | bsz 73.9 | num_updates 6304 | best_bleu 13.57
2025-11-22 07:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 6304 updates
2025-11-22 07:32:31 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 13 @ 6304 updates, score 13.57) (writing took 1.3088674610480666 seconds)
2025-11-22 07:32:32 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)                              
2025-11-22 07:32:32 | INFO | train | epoch 013 | loss 4.892 | nll_loss 3.488 | ppl 11.22 | wps 15930.6 | ups 4.92 | wpb 3240.4 | bsz 137.3 | num_updates 6304 | lr 0.000394021 | gnorm 0.84 | clip 16.5 | loss_scale 64 | train_wall 73 | gb_free 1.5 | wall 1295
2025-11-22 07:32:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:32:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 014:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:32:32 | INFO | fairseq.trainer | begin training epoch 14
2025-11-22 07:32:32 | INFO | fairseq_cli.train | Start iterating over samples
epoch 014: 100%|▉| 484/485 [01:14<00:00,  6.70it/s, loss=4.803, nll_loss=3.381, ppl=10.42, wps=20062.4, ups=6.64, wpb=30212025-11-22 07:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:33:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:34:10 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.146 | nll_loss 3.683 | ppl 12.85 | bleu 13.04 | wps 3833.8 | wpb 1765.3 | bsz 73.9 | num_updates 6789 | best_bleu 13.57
2025-11-22 07:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 6789 updates
2025-11-22 07:34:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)                              
2025-11-22 07:34:10 | INFO | train | epoch 014 | loss 4.776 | nll_loss 3.353 | ppl 10.22 | wps 16131.6 | ups 4.98 | wpb 3240.4 | bsz 137.3 | num_updates 6789 | lr 0.000424328 | gnorm 0.839 | clip 15.1 | loss_scale 64 | train_wall 73 | gb_free 1.4 | wall 1392
2025-11-22 07:34:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:34:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 015:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:34:10 | INFO | fairseq.trainer | begin training epoch 15
2025-11-22 07:34:10 | INFO | fairseq_cli.train | Start iterating over samples
epoch 015: 100%|▉| 484/485 [01:14<00:00,  6.58it/s, loss=4.756, nll_loss=3.325, ppl=10.02, wps=21107.4, ups=6.54, wpb=32282025-11-22 07:35:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:35:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:35:48 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.102 | nll_loss 3.629 | ppl 12.37 | bleu 14.39 | wps 3634.3 | wpb 1765.3 | bsz 73.9 | num_updates 7274 | best_bleu 14.39
2025-11-22 07:35:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 7274 updates
2025-11-22 07:35:48 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:35:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:35:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 15 @ 7274 updates, score 14.39) (writing took 1.3800097969942726 seconds)
2025-11-22 07:35:50 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)                              
2025-11-22 07:35:50 | INFO | train | epoch 015 | loss 4.676 | nll_loss 3.234 | ppl 9.41 | wps 15715.1 | ups 4.85 | wpb 3240.4 | bsz 137.3 | num_updates 7274 | lr 0.000454634 | gnorm 0.839 | clip 16.9 | loss_scale 64 | train_wall 73 | gb_free 1.5 | wall 1492
2025-11-22 07:35:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:35:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 016:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:35:50 | INFO | fairseq.trainer | begin training epoch 16
2025-11-22 07:35:50 | INFO | fairseq_cli.train | Start iterating over samples
epoch 016: 100%|▉| 484/485 [01:14<00:00,  6.21it/s, loss=4.688, nll_loss=3.244, ppl=9.47, wps=21206.2, ups=6.68, wpb=3173.2025-11-22 07:37:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:37:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:37:28 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.084 | nll_loss 3.597 | ppl 12.1 | bleu 15.16 | wps 3719.9 | wpb 1765.3 | bsz 73.9 | num_updates 7759 | best_bleu 15.16
2025-11-22 07:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 7759 updates
2025-11-22 07:37:28 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:37:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 16 @ 7759 updates, score 15.16) (writing took 1.33370219997596 seconds)
2025-11-22 07:37:29 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)                              
2025-11-22 07:37:29 | INFO | train | epoch 016 | loss 4.578 | nll_loss 3.12 | ppl 8.69 | wps 15801.1 | ups 4.88 | wpb 3240.4 | bsz 137.3 | num_updates 7759 | lr 0.000484941 | gnorm 0.84 | clip 15.1 | loss_scale 64 | train_wall 73 | gb_free 1.4 | wall 1592
2025-11-22 07:37:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:37:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 017:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:37:29 | INFO | fairseq.trainer | begin training epoch 17
2025-11-22 07:37:29 | INFO | fairseq_cli.train | Start iterating over samples
epoch 017: 100%|▉| 484/485 [01:14<00:00,  6.58it/s, loss=4.546, nll_loss=3.08, ppl=8.45, wps=21473.2, ups=6.5, wpb=3305.3,2025-11-22 07:38:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:38:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:39:08 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.041 | nll_loss 3.554 | ppl 11.75 | bleu 14.8 | wps 3644.7 | wpb 1765.3 | bsz 73.9 | num_updates 8244 | best_bleu 15.16
2025-11-22 07:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 8244 updates
2025-11-22 07:39:08 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_14.8000.pt
2025-11-22 07:39:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_14.8000.pt
2025-11-22 07:39:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint.best_bleu_14.8000.pt (epoch 17 @ 8244 updates, score 14.8) (writing took 0.6713545090169646 seconds)
2025-11-22 07:39:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)                              
2025-11-22 07:39:08 | INFO | train | epoch 017 | loss 4.488 | nll_loss 3.013 | ppl 8.07 | wps 15823.4 | ups 4.88 | wpb 3240.4 | bsz 137.3 | num_updates 8244 | lr 0.000492545 | gnorm 0.845 | clip 17.3 | loss_scale 64 | train_wall 73 | gb_free 1.7 | wall 1691
2025-11-22 07:39:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:39:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 018:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:39:08 | INFO | fairseq.trainer | begin training epoch 18
2025-11-22 07:39:08 | INFO | fairseq_cli.train | Start iterating over samples
epoch 018:  18%|▏| 87/485 [00:13<01:02,  6.39it/s, loss=4.409, nll_loss=2.925, ppl=7.59, wps=8149.8, ups=2.47, wpb=3299, b2025-11-22 07:39:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
epoch 018: 100%|▉| 484/485 [01:14<00:00,  6.07it/s, loss=4.494, nll_loss=3.018, ppl=8.1, wps=21093.5, ups=6.6, wpb=3194.8,2025-11-22 07:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:40:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:40:46 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.039 | nll_loss 3.538 | ppl 11.62 | bleu 16.05 | wps 3795.3 | wpb 1765.3 | bsz 73.9 | num_updates 8728 | best_bleu 16.05
2025-11-22 07:40:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 8728 updates
2025-11-22 07:40:46 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:40:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 18 @ 8728 updates, score 16.05) (writing took 1.3954208039795049 seconds)
2025-11-22 07:40:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)                              
2025-11-22 07:40:47 | INFO | train | epoch 018 | loss 4.378 | nll_loss 2.886 | ppl 7.39 | wps 15851.4 | ups 4.89 | wpb 3240.4 | bsz 136.7 | num_updates 8728 | lr 0.000478694 | gnorm 0.846 | clip 15.9 | loss_scale 32 | train_wall 73 | gb_free 1.4 | wall 1790
2025-11-22 07:40:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:40:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 019:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:40:47 | INFO | fairseq.trainer | begin training epoch 19
2025-11-22 07:40:47 | INFO | fairseq_cli.train | Start iterating over samples
epoch 019: 100%|▉| 484/485 [01:13<00:00,  6.35it/s, loss=4.308, nll_loss=2.803, ppl=6.98, wps=21425.2, ups=6.57, wpb=3261.2025-11-22 07:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:42:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:42:26 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.032 | nll_loss 3.51 | ppl 11.39 | bleu 16.44 | wps 3639 | wpb 1765.3 | bsz 73.9 | num_updates 9213 | best_bleu 16.44
2025-11-22 07:42:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 9213 updates
2025-11-22 07:42:26 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:42:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:42:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 19 @ 9213 updates, score 16.44) (writing took 1.3179417150095105 seconds)
2025-11-22 07:42:27 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)                              
2025-11-22 07:42:27 | INFO | train | epoch 019 | loss 4.274 | nll_loss 2.765 | ppl 6.8 | wps 15737.7 | ups 4.86 | wpb 3240.4 | bsz 137.3 | num_updates 9213 | lr 0.000465923 | gnorm 0.845 | clip 16.3 | loss_scale 32 | train_wall 73 | gb_free 1.5 | wall 1890
2025-11-22 07:42:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:42:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 020:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:42:27 | INFO | fairseq.trainer | begin training epoch 20
2025-11-22 07:42:27 | INFO | fairseq_cli.train | Start iterating over samples
epoch 020: 100%|▉| 484/485 [01:14<00:00,  6.20it/s, loss=4.17, nll_loss=2.641, ppl=6.24, wps=21052.7, ups=6.57, wpb=3205.92025-11-22 07:43:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:43:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:44:05 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.029 | nll_loss 3.51 | ppl 11.4 | bleu 16.71 | wps 3693.3 | wpb 1765.3 | bsz 73.9 | num_updates 9698 | best_bleu 16.71
2025-11-22 07:44:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 9698 updates
2025-11-22 07:44:05 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:44:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint_best.pt
2025-11-22 07:44:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint_best.pt (epoch 20 @ 9698 updates, score 16.71) (writing took 1.3829250079579651 seconds)
2025-11-22 07:44:07 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)                              
2025-11-22 07:44:07 | INFO | train | epoch 020 | loss 4.168 | nll_loss 2.641 | ppl 6.24 | wps 15781.2 | ups 4.87 | wpb 3240.4 | bsz 137.3 | num_updates 9698 | lr 0.000454123 | gnorm 0.835 | clip 16.1 | loss_scale 32 | train_wall 73 | gb_free 1.5 | wall 1990
2025-11-22 07:44:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:44:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 021:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:44:07 | INFO | fairseq.trainer | begin training epoch 21
2025-11-22 07:44:07 | INFO | fairseq_cli.train | Start iterating over samples
epoch 021: 100%|▉| 484/485 [01:14<00:00,  6.50it/s, loss=4.11, nll_loss=2.573, ppl=5.95, wps=21901, ups=6.48, wpb=3379.4, 2025-11-22 07:45:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:45:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:45:46 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.034 | nll_loss 3.505 | ppl 11.35 | bleu 16.63 | wps 3526.3 | wpb 1765.3 | bsz 73.9 | num_updates 10183 | best_bleu 16.71
2025-11-22 07:45:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 10183 updates
2025-11-22 07:45:46 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_16.6301.pt
2025-11-22 07:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_16.6301.pt
2025-11-22 07:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint.best_bleu_16.6301.pt (epoch 21 @ 10183 updates, score 16.63) (writing took 0.6724971559597179 seconds)
2025-11-22 07:45:47 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)                              
2025-11-22 07:45:47 | INFO | train | epoch 021 | loss 4.076 | nll_loss 2.534 | ppl 5.79 | wps 15705.1 | ups 4.85 | wpb 3240.4 | bsz 137.3 | num_updates 10183 | lr 0.000443177 | gnorm 0.838 | clip 15.3 | loss_scale 32 | train_wall 73 | gb_free 1.3 | wall 2090
2025-11-22 07:45:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:45:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 022:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:45:47 | INFO | fairseq.trainer | begin training epoch 22
2025-11-22 07:45:47 | INFO | fairseq_cli.train | Start iterating over samples
epoch 022: 100%|▉| 484/485 [01:14<00:00,  6.63it/s, loss=4.098, nll_loss=2.556, ppl=5.88, wps=20489.9, ups=6.62, wpb=3094.2025-11-22 07:47:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:47:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:47:24 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.045 | nll_loss 3.513 | ppl 11.42 | bleu 15.53 | wps 3913 | wpb 1765.3 | bsz 73.9 | num_updates 10668 | best_bleu 16.71
2025-11-22 07:47:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 10668 updates
2025-11-22 07:47:24 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)                              
2025-11-22 07:47:24 | INFO | train | epoch 022 | loss 3.989 | nll_loss 2.433 | ppl 5.4 | wps 16210.8 | ups 5 | wpb 3240.4 | bsz 137.3 | num_updates 10668 | lr 0.000432986 | gnorm 0.843 | clip 16.5 | loss_scale 32 | train_wall 73 | gb_free 1.6 | wall 2187
2025-11-22 07:47:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:47:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 023:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:47:24 | INFO | fairseq.trainer | begin training epoch 23
2025-11-22 07:47:24 | INFO | fairseq_cli.train | Start iterating over samples
epoch 023: 100%|▉| 484/485 [01:14<00:00,  6.63it/s, loss=3.939, nll_loss=2.372, ppl=5.18, wps=20770.7, ups=6.58, wpb=3158.2025-11-22 07:48:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:48:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:49:02 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.03 | nll_loss 3.493 | ppl 11.26 | bleu 15.6 | wps 3659.9 | wpb 1765.3 | bsz 73.9 | num_updates 11153 | best_bleu 16.71
2025-11-22 07:49:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 11153 updates
2025-11-22 07:49:02 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)                              
2025-11-22 07:49:02 | INFO | train | epoch 023 | loss 3.91 | nll_loss 2.34 | ppl 5.06 | wps 15977.4 | ups 4.93 | wpb 3240.4 | bsz 137.3 | num_updates 11153 | lr 0.000423467 | gnorm 0.844 | clip 15.5 | loss_scale 32 | train_wall 73 | gb_free 1.4 | wall 2285
2025-11-22 07:49:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:49:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 024:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:49:02 | INFO | fairseq.trainer | begin training epoch 24
2025-11-22 07:49:02 | INFO | fairseq_cli.train | Start iterating over samples
epoch 024: 100%|▉| 484/485 [01:14<00:00,  6.46it/s, loss=3.889, nll_loss=2.314, ppl=4.97, wps=21286.7, ups=6.45, wpb=3298.2025-11-22 07:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:50:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:50:41 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.063 | nll_loss 3.534 | ppl 11.58 | bleu 16.23 | wps 3635.8 | wpb 1765.3 | bsz 73.9 | num_updates 11638 | best_bleu 16.71
2025-11-22 07:50:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 11638 updates
2025-11-22 07:50:41 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)                              
2025-11-22 07:50:41 | INFO | train | epoch 024 | loss 3.834 | nll_loss 2.251 | ppl 4.76 | wps 15952.3 | ups 4.92 | wpb 3240.4 | bsz 137.3 | num_updates 11638 | lr 0.000414549 | gnorm 0.845 | clip 15.9 | loss_scale 32 | train_wall 73 | gb_free 1.5 | wall 2384
2025-11-22 07:50:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:50:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 025:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:50:41 | INFO | fairseq.trainer | begin training epoch 25
2025-11-22 07:50:41 | INFO | fairseq_cli.train | Start iterating over samples
epoch 025: 100%|▉| 484/485 [01:14<00:00,  6.43it/s, loss=3.868, nll_loss=2.29, ppl=4.89, wps=20993.1, ups=6.58, wpb=3191, 2025-11-22 07:51:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:51:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:52:19 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.058 | nll_loss 3.533 | ppl 11.57 | bleu 16.7 | wps 3692.8 | wpb 1765.3 | bsz 73.9 | num_updates 12123 | best_bleu 16.71
2025-11-22 07:52:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 12123 updates
2025-11-22 07:52:19 | INFO | fairseq.trainer | Saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_16.7000.pt
2025-11-22 07:52:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/logyre22/Projects/nlp/machine_translation/fairseq_mt/checkpoints/augmentation/aug2/checkpoint.best_bleu_16.7000.pt
2025-11-22 07:52:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../../checkpoints/augmentation/aug2/checkpoint.best_bleu_16.7000.pt (epoch 25 @ 12123 updates, score 16.7) (writing took 0.6718604670022614 seconds)
2025-11-22 07:52:20 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)                              
2025-11-22 07:52:20 | INFO | train | epoch 025 | loss 3.768 | nll_loss 2.175 | ppl 4.51 | wps 15880.4 | ups 4.9 | wpb 3240.4 | bsz 137.3 | num_updates 12123 | lr 0.000406172 | gnorm 0.848 | clip 16.7 | loss_scale 32 | train_wall 73 | gb_free 1.6 | wall 2483
2025-11-22 07:52:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:52:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 026:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:52:20 | INFO | fairseq.trainer | begin training epoch 26
2025-11-22 07:52:20 | INFO | fairseq_cli.train | Start iterating over samples
epoch 026: 100%|▉| 484/485 [01:14<00:00,  6.35it/s, loss=3.769, nll_loss=2.175, ppl=4.52, wps=21303.3, ups=6.51, wpb=3272.2025-11-22 07:53:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:53:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:53:57 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.089 | nll_loss 3.561 | ppl 11.8 | bleu 15.71 | wps 3778.5 | wpb 1765.3 | bsz 73.9 | num_updates 12608 | best_bleu 16.71
2025-11-22 07:53:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 12608 updates
2025-11-22 07:53:57 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)                              
2025-11-22 07:53:57 | INFO | train | epoch 026 | loss 3.7 | nll_loss 2.096 | ppl 4.27 | wps 16101.9 | ups 4.97 | wpb 3240.4 | bsz 137.3 | num_updates 12608 | lr 0.000398283 | gnorm 0.848 | clip 17.1 | loss_scale 32 | train_wall 73 | gb_free 1.5 | wall 2580
2025-11-22 07:53:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:53:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 027:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:53:57 | INFO | fairseq.trainer | begin training epoch 27
2025-11-22 07:53:57 | INFO | fairseq_cli.train | Start iterating over samples
epoch 027: 100%|▉| 484/485 [01:14<00:00,  6.33it/s, loss=3.652, nll_loss=2.038, ppl=4.11, wps=21144.9, ups=6.52, wpb=3242.2025-11-22 07:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:55:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:55:35 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.115 | nll_loss 3.59 | ppl 12.04 | bleu 15.11 | wps 3742 | wpb 1765.3 | bsz 73.9 | num_updates 13093 | best_bleu 16.71
2025-11-22 07:55:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 13093 updates
2025-11-22 07:55:35 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)                              
2025-11-22 07:55:35 | INFO | train | epoch 027 | loss 3.637 | nll_loss 2.022 | ppl 4.06 | wps 16071.1 | ups 4.96 | wpb 3240.4 | bsz 137.3 | num_updates 13093 | lr 0.000390837 | gnorm 0.845 | clip 15.5 | loss_scale 32 | train_wall 73 | gb_free 1.5 | wall 2678
2025-11-22 07:55:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:55:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 028:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:55:35 | INFO | fairseq.trainer | begin training epoch 28
2025-11-22 07:55:35 | INFO | fairseq_cli.train | Start iterating over samples
epoch 028: 100%|▉| 484/485 [01:14<00:00,  6.54it/s, loss=3.587, nll_loss=1.963, ppl=3.9, wps=21756.2, ups=6.44, wpb=3378.22025-11-22 07:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:56:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:57:14 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.121 | nll_loss 3.594 | ppl 12.08 | bleu 15.85 | wps 3627 | wpb 1765.3 | bsz 73.9 | num_updates 13578 | best_bleu 16.71
2025-11-22 07:57:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 13578 updates
2025-11-22 07:57:14 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)                              
2025-11-22 07:57:14 | INFO | train | epoch 028 | loss 3.58 | nll_loss 1.956 | ppl 3.88 | wps 15943 | ups 4.92 | wpb 3240.4 | bsz 137.3 | num_updates 13578 | lr 0.000383793 | gnorm 0.845 | clip 15.7 | loss_scale 32 | train_wall 73 | gb_free 1.4 | wall 2776
2025-11-22 07:57:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:57:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 029:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:57:14 | INFO | fairseq.trainer | begin training epoch 29
2025-11-22 07:57:14 | INFO | fairseq_cli.train | Start iterating over samples
epoch 029: 100%|▉| 484/485 [01:14<00:00,  6.34it/s, loss=3.591, nll_loss=1.966, ppl=3.91, wps=21357.5, ups=6.56, wpb=3253.2025-11-22 07:58:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 07:58:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 07:58:52 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.164 | nll_loss 3.63 | ppl 12.38 | bleu 14.89 | wps 3657.3 | wpb 1765.3 | bsz 73.9 | num_updates 14063 | best_bleu 16.71
2025-11-22 07:58:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 14063 updates
2025-11-22 07:58:52 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)                              
2025-11-22 07:58:52 | INFO | train | epoch 029 | loss 3.525 | nll_loss 1.892 | ppl 3.71 | wps 15977.8 | ups 4.93 | wpb 3240.4 | bsz 137.3 | num_updates 14063 | lr 0.000377117 | gnorm 0.854 | clip 16.1 | loss_scale 32 | train_wall 73 | gb_free 1.5 | wall 2875
2025-11-22 07:58:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2025-11-22 07:58:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 485
epoch 030:   0%|                                                                                  | 0/485 [00:00<?, ?it/s]2025-11-22 07:58:52 | INFO | fairseq.trainer | begin training epoch 30
2025-11-22 07:58:52 | INFO | fairseq_cli.train | Start iterating over samples
epoch 030: 100%|▉| 484/485 [01:14<00:00,  6.37it/s, loss=3.559, nll_loss=1.93, ppl=3.81, wps=21316.9, ups=6.52, wpb=3268.82025-11-22 08:00:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-22 08:00:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
                                                                                                                         2025-11-22 08:00:30 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.164 | nll_loss 3.648 | ppl 12.53 | bleu 14.01 | wps 3654.1 | wpb 1765.3 | bsz 73.9 | num_updates 14548 | best_bleu 16.71