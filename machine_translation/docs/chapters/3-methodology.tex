\section{Methodology}

\subsection{Data Representation and Tokenization}

Philippine languages are considered morphologically rich due to the extensive use of affixes to describe tenses and generate new words that are akin to the original definition of the root word. For example, the word \texttt{gawa} can turn into \texttt{pagawa}, \texttt{gumawa}, \texttt{pagawaan}, \texttt{magagawa}. With just a few affixes, the words meaning changes entirely. This level of morphology makes it difficult to train a model if it were to identify each and every word as a completely separate token as it would mean ignoring the shared structure and unnecessarily increasing the vocabulary.
\\

Deciding in which way we process these data is important in training the model and how effective will the model be at translation. Due to the nature of the Philippine languages' morphologically rich structure, subword tokenization allows us to properly analyze the structure of the word and dissect the root word and its corresponding affixes.
\\

Among the numerous subword tokenization strategies, we chose unigram tokenization. This method addresses the Philippine languages' morphologically rich structure. Unigram tokenization addresses this by utilizing a probabilistic algorithm based on frequency that selectively chooses a set of subwords that reaches the target size of the vocabulary.
\\

As an alternative, we had considered BPE; however, its method in frequently merging the subwords pairs together aligns more for a large corpora. This would not be applicable to our small corpus which would lead to a vocabulary that is overly small and insufficient for analyzing Filipino languages' rich morphology.

\subsubsection{SentencePiece Training}

We implemented unigram tokenization using the SentencePiece library, training a single shared vocabulary across all language pairs. The training process reads all raw training data from our parallel corpora, concatenating them into a single file to cover the character distributions across languages.
\\

The SentencePiece model was trained with the following configuration: vocabulary size of 4,000 subword units, character coverage of 1.0 to capture all Unicode characters in Philippine languages, and unigram model type for probabilistic subword selection. We also incorporated user-defined symbols corresponding to language tags (\texttt{<bik>}, \texttt{<ceb>}, \texttt{<tgl>}, etc.) to enable multi-language processing.
\\

This shared vocabulary approach has several advantages. First, it reduces the model's complexity by using a single embedding layer for all languages. Second, it facilitates transfer learning between related languages. Third, it naturally handles code-switching and cognates that appear across Philippine languages.
\\

After training, we encoded all training, validation, and test files using the learned SentencePiece model. Each sentence is tokenized into subword units, which are then space-separated for Fairseq preprocessing. This encoding step transforms the human-readable text into model-consumable subword sequences while preserving the alignment between parallel sentences.

\subsection{Model Architecture}

As a majority of the group is new to machine learning, we initially looked into Fairseq models and how its applications are in machine translation. Through this, we had found transformers and its self-attention mechanisms that allow parallel processing. This information was crucial as our group faced limitations by our computing power, thus increasing the time it takes to train each model for testing.
\\

We had tested multiple different Fairseq and transformer models such as mBART50 and several different Meta models with millions of parameters to attempt to generate a stable model that translated Cebuano to Tagalog. However, intial attempts indicate that there are significant challenges in training models with a low corpora.
\\

After experiencing these challenges first hand, we searched for alternative solutions that better fit our circumstances. Due to this, we had found and experimented on a transformer model made by Meta, \texttt{transformer\_iwslt\_de\_en}, which has 1024 parameters. It is important to note that this is a notably smaller model compared to most available modern models nowadays. We had chosen this model to face the aforementioned computing and time constraints.
\\

The \texttt{transformer\_iwslt\_de\_en} architecture is specifically designed for low-resource translation tasks, originally developed for the IWSLT German-English translation shared task. The model uses a standard encoder-decoder transformer structure with reduced dimensions compared to the base transformer. This smaller architecture prevents overfitting on our limited training data while maintaining the self-attention mechanisms necessary to model the dependencies across different word structures.

\subsection{Training Configuration}

We used the following hyperparameters across all experiments:
\\

\textbf{Optimization:} We used the Adam optimizer with beta parameters (0.9, 0.98) and gradient clipping at norm 1.0 to prevent exploding gradients. The learning rate was set to 0.0005 with an inverse square root learning rate scheduler and 8,000 warmup steps for the direct translation approach (reduced to 4,000 for pivot training). The warmup phase gradually increases the learning rate from $1 \times 10^{-7}$ to the target rate, preventing instability during early training.
\\

\textbf{Regularization:} To combat overfitting on our small corpus, we applied dropout at 0.3 and weight decay of 0.0001. We also used label smoothing at 0.1, which softens the training targets and improves generalization.
\\

\textbf{Training Procedure:} We trained for a maximum of 50 epochs with a batch size of 4,096 tokens. The training itself used mixed precision (FP16) to reduce memory consumption and accelerate computation. We evaluated BLEU scores on the validation set after each epoch, saving only the top 3 checkpoints based on validation BLEU. The best checkpoint is selected based on maximum BLEU score.
\\

\textbf{Embedding Sharing:} We enabled \texttt{share-all-embeddings}, which ties the source embedding, target embedding, and output projection layers. This parameter sharing is justified by the high linguistic similarity between Cebuano and Tagalog (orthographic similarity 0.8877), allowing the model to use their shared vocabulary and morphological patterns.

\subsection{Translation Strategies}

We used two translation strategies to evaluate the tradeoff between linguistic similarity and resource availability.

\subsubsection{Direct Translation: Cebuano → Tagalog}

The direct translation approach trains a single model on the Cebuano-Tagalog parallel corpus. This strategy leverages the high orthographic (0.8877) and phonetic (0.9698) similarity between the languages. The model learns direct correspondences between Cebuano and Tagalog subwords, potentially capturing systematic morphological transformations.
\\

Training was conducted using the script \texttt{train\_src\_dest.bash}, which preprocesses the data, initializes the transformer model, and trains until convergence or the maximum epoch limit. The direct approach maximizes the exposure to the specific language pair but is constrained by the limited corpus size.

\subsubsection{Pivot Translation: Cebuano → English → Tagalog}

The pivot translation approach trains two separate models: one for Cebuano → English and another for English → Tagalog. At inference time, Cebuano input is first translated to English using the first model, then the English output is translated to Tagalog using the second model. This approach is implemented in \texttt{train\_src\_pivot\_dest.bash}, which sequentially trains both models.
\\

While pivot translation introduces compounding errors (mistakes from both models accumulate), it benefits from substantially larger training corpora for each language pair. English Bible translations are extensively parallel with both Cebuano and Tagalog, providing more training examples than the direct Cebuano-Tagalog corpus.

\subsection{Preprocessing Pipeline}

The complete preprocessing pipeline consists of four stages: (1) parallel corpus extraction from TSV files with train/validation/test splitting, (2) SentencePiece model training on concatenated training data, (3) subword encoding of all data splits using the trained model, and (4) Fairseq preprocessing to create binary vocabularies and data tensors.
\\

The Fairseq preprocessing step (\texttt{fairseq-preprocess}) creates a data-bin directory containing dictionary files and tensorized data. We used the \texttt{--joined-dictionary} flag to ensure source and target languages share vocabulary indices, which is necessary for embedding sharing. The preprocessing runs with 8 workers for parallel data loading.
\\

This pipeline is fully reproducible through the provided bash scripts in the \texttt{fairseq\_mt/scripts} directory. The modular design allows easy experimentation with different language pairs by changing the source and target language variables in the training scripts.
