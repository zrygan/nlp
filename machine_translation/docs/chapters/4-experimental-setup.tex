\section{Experimental Setup and Evaluation}

This section describes the experimental design, evaluation metrics, and testing procedures used to assess our machine translation models.

\subsection{Experimental Design}

\subsubsection{Vocabulary Size Experiments}

To analyze the impact of vocabulary size on translation quality, we trained models with different SentencePiece vocabulary configurations. We experimented with vocabulary sizes of 1,000 and 4,000 subword units. The smaller vocabulary (1k) tests whether aggressive subword compression can capture essential morphological patterns with minimal vocabulary, while the larger vocabulary (4k) provides more granular subword representations that may better preserve semantic distinctions.

\subsubsection{Tokenization Strategy Comparison}

In addition to unigram tokenization, we implemented a baseline using Byte Pair Encoding (BPE) for comparison. This allows us to evaluate whether the probabilistic unigram approach offers advantages over the merge-based BPE algorithm for morphologically rich Philippine languages. The BPE model uses similar preprocessing and training procedures to isolate the effect of tokenization strategy.

\subsection{Evaluation Metrics}

\subsubsection{BLEU Score}

The primary evaluation metric is BLEU4 (Bilingual Evaluation Understudy with 4-grams), which measures n-gram precision between machine-generated translations and reference translations. BLEU scores range from 0 to 100, with higher scores indicating better translation quality. We compute BLEU using the SacreBLEU implementation to ensure reproducibility and comparability with other research.
\\

BLEU4 considers unigram, bigram, trigram, and 4-gram matches, providing a balanced assessment of both lexical choice and phrase-level fluency. The metric also includes a brevity penalty (BP) to discourage overly short translations. While BLEU has known limitations—particularly insensitivity to semantic equivalence and bias toward literal translations—it remains the standard for automatic MT evaluation and enables comparison with prior work on low-resource translation.

\subsection{Evaluation Procedure}

\subsubsection{Test Set Generation}

The test set consists of held-out Bible verses that were separated during the initial train/validation/test split (80\%/10\%/10\%). This ensures no overlap between training and evaluation data. The test set contains approximately 1,550 aligned verse pairs for Cebuano-Tagalog.
\\

For pivot translations, we evaluate each stage independently (Cebuano→English and English→Tagalog) and compute the end-to-end translation quality by pipelining the models. The intermediate English translations are saved for inspection and error analysis.

\subsubsection{Beam Search Decoding}

At inference time, we use beam search with a beam width of 5 to generate translations. Beam search maintains multiple hypotheses at each decoding step, selecting the 5 most probable continuations. This improves translation quality compared to greedy decoding, which selects only the single most probable token at each step.

\subsection{Hardware and Software Environment}

All experiments were conducted on consumer-grade hardware using Fairseq 0.12.2, PyTorch 2.0.1, Python 3.11, and SentencePiece 0.1.99. The training itself used mixed precision (FP16) and took 10-15 minutes per model, demonstrating that low-resource MT systems can be developed without large-scale computational infrastructure.

\subsection{Gold Standard and Reference Translations}

Our gold standard references are the human-translated Bible verses from the original parallel corpus. These translations were produced by professional translators and religious scholars.

\subsection{Reproducibility}

All experiments are fully reproducible through the provided scripts in the \texttt{fairseq\_mt/scripts} directory with a fixed random seed 42. Model checkpoints and evaluation outputs are preserved in the \texttt{fairseq\_mt/evaluation} directory, organized by experiment name.
