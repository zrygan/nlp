{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"google/mt5-small\"\n",
    "ADAPTER_SAVE_DIR = \"./mt5-ceb-tl-lora-final\"\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD DATASET\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"csv\", data_files=\"ceb_tgl.tsv\", delimiter=\"\\t\")\n",
    "dataset = dataset.rename_column(\"source_text\", \"src\")\n",
    "dataset = dataset.rename_column(\"target_text\", \"tgt\")\n",
    "\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "# -----------------------------\n",
    "# TOKENIZER & MODEL\n",
    "# -----------------------------\n",
    "tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# -----------------------------\n",
    "# APPLY LoRA\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"],  # all attention projections\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "model.print_trainable_parameters()  # should be non-zero\n",
    "\n",
    "# -----------------------------\n",
    "# PREPROCESS FUNCTION WITH ERROR HANDLING\n",
    "# -----------------------------\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"src\"], max_length=32, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(batch[\"tgt\"], max_length=32, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    new_labels = []\n",
    "    for l in labels[\"input_ids\"]:\n",
    "        new_l = [token if token != tokenizer.pad_token_id else -100 for token in l]\n",
    "        # Force at least one token\n",
    "        if all(t == -100 for t in new_l):\n",
    "            new_l[0] = tokenizer.eos_token_id\n",
    "        new_labels.append(new_l)\n",
    "\n",
    "    inputs[\"labels\"] = new_labels\n",
    "    return inputs\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_eval = eval_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# -----------------------------\n",
    "# DATA COLLATOR & TRAINING ARGS\n",
    "# -----------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-ceb-tl-lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# BLEU METRIC\n",
    "# -----------------------------\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def safe_decode(ids, tokenizer):\n",
    "    # Replace -100 with pad_token_id\n",
    "    ids = [token if 0 <= token < tokenizer.vocab_size else tokenizer.pad_token_id for token in ids]\n",
    "    ids = [token if token != -100 else tokenizer.pad_token_id for token in ids]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def compute_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # Safe decode predictions\n",
    "    decoded_preds = [safe_decode(p, tokenizer) for p in preds]\n",
    "\n",
    "    # Safe decode labels\n",
    "    decoded_labels = [[safe_decode(l, tokenizer)] for l in labels]  # sacrebleu expects list of references\n",
    "\n",
    "    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINER\n",
    "# -----------------------------\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Input:\", tokenizer.decode(tokenized_train[i][\"input_ids\"], skip_special_tokens=True))\n",
    "    safe_labels = [t if t != -100 else tokenizer.pad_token_id for t in tokenized_train[i][\"labels\"]]\n",
    "    print(\"Label:\", tokenizer.decode(safe_labels, skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN\n",
    "# -----------------------------\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SAVE LoRA ADAPTER\n",
    "# -----------------------------\n",
    "model.save_pretrained(ADAPTER_SAVE_DIR)\n",
    "print(\"LoRA adapter saved at:\", ADAPTER_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0023b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n",
      "\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "\n",
      "Loading LoRA adapter from: ./adapter\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at './adapter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\clare\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\config.py:317\u001b[39m, in \u001b[36mPeftConfigMixin._get_peft_type\u001b[39m\u001b[34m(cls, model_id, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     config_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\clare\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\clare\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m The name cannot start or end with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and the maximum length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: './adapter'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# LOAD LORA ADAPTER\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLoading LoRA adapter from:\u001b[39m\u001b[33m\"\u001b[39m, ADAPTER_DIR)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mADAPTER_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m model.to(DEVICE)\n\u001b[32m     41\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\clare\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\peft_model.py:459\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_auth_token := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33muse_auth_token\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    458\u001b[39m         hf_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_auth_token\u001b[39m\u001b[33m\"\u001b[39m] = use_auth_token\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     config = PEFT_TYPE_TO_CONFIG_MAPPING[\u001b[43mPeftConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhf_kwargs\u001b[49m\u001b[43m)\u001b[49m].from_pretrained(\n\u001b[32m    460\u001b[39m         model_id, **kwargs\n\u001b[32m    461\u001b[39m     )\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[32m    463\u001b[39m     config.inference_mode = \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\clare\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\config.py:323\u001b[39m, in \u001b[36mPeftConfigMixin._get_peft_type\u001b[39m\u001b[34m(cls, model_id, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m         config_file = hf_hub_download(\n\u001b[32m    318\u001b[39m             model_id,\n\u001b[32m    319\u001b[39m             CONFIG_NAME,\n\u001b[32m    320\u001b[39m             **hf_hub_download_kwargs,\n\u001b[32m    321\u001b[39m         )\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    325\u001b[39m loaded_attributes = \u001b[38;5;28mcls\u001b[39m.from_json_file(config_file)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[33m\"\u001b[39m\u001b[33mpeft_type\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Can't find 'adapter_config.json' at './adapter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIG - CHANGE THESE\n",
    "# ---------------------------------------------------------\n",
    "BASE_MODEL = \"google/mt5-small\"              # or mt5-base (must match what you trained on)\n",
    "ADAPTER_DIR = \"./adapter\"                    # path to folder with adapter_model.safetensors\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"DEVICE =\", DEVICE)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD TOKENIZER\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD BASE MODEL\n",
    "# ---------------------------------------------------------\n",
    "print(\"Loading base model...\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD LORA ADAPTER\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nLoading LoRA adapter from:\", ADAPTER_DIR)\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    ADAPTER_DIR,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# VERIFY LORA SUCCESSFULLY LOADED\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n=== Trainable Parameters Check ===\")\n",
    "model.print_trainable_parameters()\n",
    "# EXPECTED OUTPUT:\n",
    "# trainable params: something NON-ZERO\n",
    "# If it's 0 â†’ your adapter folder is wrong\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# INFERENCE FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def generate(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RUN TESTS\n",
    "# ---------------------------------------------------------\n",
    "tests = [\n",
    "    \"unsa imong pangalan\",\n",
    "    \"ganahan ko mokaon og isda\",\n",
    "    \"kumusta ka\",\n",
    "    \"asa ka gikan\",\n",
    "]\n",
    "\n",
    "print(\"\\n===== TEST RESULTS =====\")\n",
    "for t in tests:\n",
    "    print(f\"\\nInput: {t}\")\n",
    "    print(\"Output:\", generate(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50579dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 300,520,832 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
