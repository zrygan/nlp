\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, listings, cleveref, graphicx}
\usepackage{pdflscape}
\usepackage{tablefootnote}
\usepackage[edges]{forest}
\usepackage{todonotes}
\usepackage{xcolor}

\input{assets/listing.tex}

\title{Construction of Biblical Corpora by Web Scraping and Extraction}
\author{Clarence Ang, Clive Ang, Roan Campo, Zhean Ganituen}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\section{Data Selection}

\subsection{Philippine Languages}

The goal of the project is to construct a corpus from the translations of The
Bible in \( n = 16 \) different Philippine Languages. The selected Philippine
languages are enumerated in~\cref{tab:ph_languages}.

\begin{table}[h!]
    \centering
    \begin{tabular}{lc}
        \hline
        \textbf{Philippine Language} & \textbf{ISO 639} \\
        \hline
        Tagalog                      & \texttt{tgl}     \\
        Cebuano                      & \texttt{ceb}     \\
        Ilocano                      & \texttt{ilo}     \\
        Hiligaynon                   & \texttt{hil}     \\
        Bikol language~\footnotemark & \texttt{bik}     \\
        Waray-Waray                  & \texttt{war}     \\
        Kapampangan                  & \texttt{pam}     \\
        Pangasinan                   & \texttt{pag}     \\
        Adasen                       & \texttt{tiu}     \\
        Chavacano                    & \texttt{cbk}     \\
        Paranan                      & \texttt{prf}     \\
        Tausug                       & \texttt{tsg}     \\
        Romblomanon                  & \texttt{rol}     \\
        Masbatenyo                   & \texttt{msb}     \\
        Kinaray-a                    & \texttt{krj}     \\
        Yami language                & \texttt{tao}     \\
        \hline
    \end{tabular}
    \caption{Philippine languages with their ISO 639 codes}\label{tab:ph_languages}
\end{table}
\footnotetext{The code \texttt{bcl} is also used for the Bikol language.}

To allow for the corpus to be inclusive and enclose a large array of
Austronesian sub-groups

These languages were selected in such a way that allows for the corpus to be
inclusive and enclose a large array of Austronesian sub-groups. The language
family or phylogenic tree of the sample languages are
on~\cref{fig:phylo_tree,fig:chavacano_phylo}. As an aside Chavacano is a
Spanish-based creole (hence, it is not Austronesian) while the Yami language is
a language of the Taiwanese indigenous people.

The ISO 639 codes and the Language families are from Ethnologue~\cite{ESF25}.
\newpage \thispagestyle{empty} \input{assets/phylogenic_tree.tex} \newpage% removes page number for this page

\subsection{Biblical Text}

The online Bible platform YouVersion (previously known as Bible.com or Bible
App) was used as the source of biblical text. To ensure a sufficiently large
corpus for each language, we set the minimum corpus size (\( |C| \)) to be
50,000 tokens. the complete English Bible contains approximately 760,000 words,
this threshold is suitable because some translations are incomplete or still in
progress~\cite{JJS11}. Examples of unfinished translations include those for
the Tausug, Paranan, and Masbatenyo languages.

\begin{table}[h!]
    \centering
    \begin{tabular}{ll}
        \hline
        \textbf{Language} & \textbf{URL (to Genesis 1)}                           \\
        \hline
        Tagalog           & \url{https://www.bible.com/bible/2195/GEN.1.ABTAG01}  \\
        Cebuano           & \url{https://www.bible.com/bible/562/GEN.1.RCPV}      \\
        Ilokano           & \url{https://www.bible.com/bible/782/GEN.1.RIPV}      \\
        Hiligaynon        & \url{https://www.bible.com/bible/2190/GEN.1.MBBHIL12} \\
        Bikol             & \url{https://www.bible.com/bible/890/GEN.1.MBBBIK92}  \\
        Waray-Waray       & \url{https://www.bible.com/bible/2198/GEN.1.MBBSAM}   \\
        Kapampangan       & \url{https://www.bible.com/bible/1141/GEN.1.PMPV}     \\
        Pangasinan        & \url{https://www.bible.com/bible/2194/GEN.1.MBBPAN83} \\
        Adasen            & \url{https://www.bible.com/bible/2812/MAT.1.YBT}      \\
        Chavacano         & \url{https://www.bible.com/bible/1129/MAT.1.CBKNT}    \\
        Paranan           & \url{https://www.bible.com/bible/438/MAT.1.PRF}       \\
        Tausug            & \url{https://www.bible.com/bible/1319/MAT.1.TSG}      \\
        Romblomanon       & \url{https://www.bible.com/bible/2244/MAT.1.BKR}      \\
        Masbatenyo        & \url{https://www.bible.com/bible/1222/MAT.1.MSB}      \\
        Kinaray-a         & \url{https://www.bible.com/bible/1489/MAT.1.KRJNT}    \\
        Yami language     & \url{https://www.bible.com/bible/2364/MAT.1.SNT}      \\
        \hline
    \end{tabular}
    \caption{Links to Genesis/Matthew translations in various languages.}
\end{table}

\section{Corpus Construction}

The source code of the methodology may be accessed in this GitHub repository:
\url{https://github.com/zrygan/zrygan.nlp/tree/master/bible_cleaning}

\subsection{Web Scraping and Parsing}

Since YouVersion is an online platform written in the Hypertext Markup Language
(HTML). We started by analyzing the structure of each webpage, pinning down the
location of the relevant biblical content in each page. In this case, we need
to find the location of the verses of each chapter in the HTML source.

Doing a manual scan of the source code, we determined that all spans with
\texttt{ChapterContent\_verse\_\_} as the prefix of its class is biblical text.

The listing below contains an excerpt of the source code with the specific tag
which contains a verse of that chapter. The \cref{list:chapter_content} is the
first verse of Genesis chapter 1.

\begin{lstlisting}[language=HTML, caption={\texttt{ChapterContent\_verse\_\_} tag}, label={list:chapter_content}]
<span 
    data-usfm="GEN.1.1"
    class="ChapterContent_verse__57FIw">
    
    <span class="ChapterContent_label__R2PLt">
        1
    </span>
    
    <span class="ChapterContent_content__RrUqA">
        Nang pasimula, nilikha ng Diyos ang
        langit
    </span>
    
    <span class="ChapterContent_note__YlDW0">
        <!-- content note -->
    </span>
    
    <span class="ChapterContent_content__RrUqA">
        at ang lupa.
    </span>
</span>
\end{lstlisting}

The notation we will use to refer to a specific biblical chapter and a verse is
\[\texttt{ChapterShorthand ChapterNumber:VerseNumber}\]
For example, the above chapter and verse in the above listing will be referred
to as Gen 1:1. For a range of verses we will instead use
\[\texttt{ChapterShorthand ChapterNumber:RangeStart-RangeEnd}\]
For example, we will use Gen 1:1-3 to refer to the first three verses of
Genesis chapter 1.

Furthermore, we will only extract all those children-spans whose class is
prefixed with \texttt{ChapterContent\_content\_\_}. While we will discard the
rest. For instance, we will disregard the \texttt{CharcterContent\_note\_} tag
which are notes provided by YouVersion to the reader.

To make the solution efficient, the web scraping and parsing step occur
simultaneously. The web scraper uses the Colly framework for Go (or Golang).
While the parsing uses a combination of string search functions and regular
expressions for cleaning.

On~\cref{list:parse_and_clean}, we scrape the HTML source code and find all
instances of the desired tag. We then extract and clean the contents within
that tag. The code uses a couple of regular expressions (RE) for cleaning which
are enumerated in~\cref{tab:regex}.

\begin{lstlisting}[language=Go, caption={Parsing and Cleaning Step}, label={list:parse_and_clean}]
c := colly.NewCollector()

c.OnHTML("span[class^='ChapterContent_verse__']", func(e *colly.HTMLElement) {
    verseNumber := e.ChildText("span[class^='ChapterContent_label__']")
    verseNumber = regexp.MustCompile(`\D`).ReplaceAllString(verseNumber, "")
    var verseTexts []string
    e.DOM.Find("span[class^='ChapterContent_content__']").Each(func(_ int, s *goquery.Selection) {
        text := strings.TrimSpace(s.Text())
        if text != "" {
            clean := text
            for _, re := range cleaningRes {
                clean = re.ReplaceAllString(clean, "")
            }
            if clean != "" {
                verseTexts = append(verseTexts, clean)
            }
        }
    })
    if len(verseTexts) > 0 {
        verses = append(verses, fmt.Sprintf("%s: %s", verseNumber, strings.Join(verseTexts, " ")))
    }
})
\end{lstlisting}

\begin{table}[h!]
    \centering
    \begin{tabular}{p{\dimexpr0.5\linewidth\relax} p{\dimexpr0.4\linewidth\relax}}
        \hline
        \textbf{Regular Expression (RE)}        & \textbf{Purpose}                                                                      \\
        \hline
        \verb|[^a-zA-Z0-9\s\.\,\;\:\!\?\'\"-]+| & Remove all characters that are not letters, digits, whitespace, or basic punctuation. \\
        \verb|[:\s]+$|                          & Match one or more colons or spaces at the end of a line.                              \\
        \verb|^[\d#:\s]+|                       & Remove leading numbers, hashes, colons, or spaces at the start of a line.             \\
        \hline
    \end{tabular}
    \caption{REs used for text cleaning}\label{tab:regex}
\end{table}

To allow for the algorithm to continuously run, we now needed to find the
location of the next Bible chapter in the webpage. Again, the approach is
straightforward and we simply needed to determine the tag or class containing
the hyperlink to the next page.

We found that any \texttt{<a>} tag with the \texttt{href} leading to a
\texttt{/bible/} page are navigation buttons. So, we simply created a
\texttt{visited} hash table containing all visited hyperlinks. And all
\texttt{<a>} tags are cross-referenced with that hash table. Excerpts of this
routine are found in~\cref{list:next_chap,list:open_next}

\begin{lstlisting}[language=Go, caption={Finding the next chapter using the HTML tags}, label={list:next_chap}]
c := colly.NewCollector()

c.OnHTML("a[href^='/bible/']", func(e *colly.HTMLElement) {
    title := e.DOM.Find("svg title").Text()
    if title == "Next Chapter" {
        nextURL = "https://www.bible.com" + e.Attr("href")
    }
})    
\end{lstlisting}

\begin{lstlisting}[language=Go, caption={Opening the next chapter}, label={list:open_next}]
if nextURL != "" && !visited[nextURL] {
    wordCount += ScrapeAndParse(nextURL,
    language,
    outputDir, cleaningRes,
    visited,
    chapterCounter,
    maxCount,
    )
}
\end{lstlisting}

Finally, concurrency was used to make the algorithm efficient.

\section{Segmenting the Corpus}

\subsection{Verse Segmenting}
The verse segmenting is done during the web scraping step. Each verse has
regular expressions applied in order to clean the text. The regular expressions
used are enumerated in~\cref{tab:regex}.

\subsection{Sentence Segmenting}
The sentence segmenting takes the existing verse segmentation after being web
scraped and further segments it into sentences by using a regular expression
that matches sentence-ending punctuation marks such as periods, exclamation
marks, and question marks. Cases such as titles were accounted for this in this
regular expression in order to avoid incorrect segmentation.

\section{Parallel Corpus}

\subsection{Indexing and Methodology}

Indexing was used in order to align the verses of each language. The index was
obtained by applying a RegEx on the each respective file in order to determine
the language prefix, the code for the book, the book name, and the verse
number. Through this, we were able to create a unique map that allows us to
align the verses of each language.\\

After which, we created a new file for each language that contains the verses
that are aligned with the other languages. This was done by iterating through
the index and checking if the verse exists in the respective language file. If
it does, we append it to the new file.\\

\bibliography{refs}
\bibliographystyle{alpha}

\vfill
{\footnotesize
    \noindent\textbf{Declaration of Generative AI Use} \\
    The authors affirm that generative artificial intelligence (AI) tools were used only to assist in improving language quality and recalling standard programming concepts. All AI-generated content was thoroughly reviewed, verified, and edited by the authors, who take full responsibility for the accuracy, originality, and integrity of the final manuscript.

    \medskip
    \noindent\textbf{Clarence Ang} \\
    \emph{Extent of use:} Minimal. Utilized solely for grammar refinement and verification of programming concepts. \\
    \emph{Example prompt:}
    \begin{itemize}
        \item ``Explain the following Go code: [code snippet]''
    \end{itemize}

    \noindent\textbf{Clive Ang} \\
    \emph{Extent of use:} FIXME. \\
    \emph{Example prompts:}
    \begin{itemize}
        \item ``FIXME''
    \end{itemize}

    \noindent\textbf{Roan Campo} \\
    \emph{Extent of use:} Minimal. Solely used for AI-assisted recall of programming sytax and concepts and verification of programming concepts. \\
    \emph{Example prompts:}
    \begin{itemize}
        \item ``Explain the following Go code: [code snippet]''
        \item ``How do you implement [specific task] in [programming language]? Refer to official documentation.''
    \end{itemize}

    \noindent\textbf{Zhean Ganituen} \\
    \emph{Extent of use:} Minimal. Applied exclusively for grammatical correction, enhancement of flow and clarity, and AI-assisted recall of programming syntax and concepts. \\
    \emph{Example prompts:}
    \begin{itemize}
        \item ``Improve the grammar and clarity of the text below without altering its meaning.''
        \item ``How do you implement [specific task] in [programming language]? Refer to the official documentation.''
    \end{itemize}
    \par}

\end{document}